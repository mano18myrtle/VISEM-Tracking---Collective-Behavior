{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5adf4106-1b58-429a-b1f6-ad3f34e19233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\asue\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\asue\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5546c-8805-4d67-abeb-6662fa76085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d20334-4f03-4b05-871e-ce4cd95c9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fccfc6-634d-4029-9acb-320f54b4a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3810c-dddd-439f-b094-5a2851d06ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4cc90-26e3-49c1-afdb-ab0c6473ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0497c65-6e50-4926-9ab7-b0ed860d0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2320c6e3-f253-4746-b4b7-23c22b074743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\asue\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asue\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pandas in c:\\users\\asue\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asue\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy opencv-python pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4a5e5-fd66-44ec-89fa-7d7ea83e7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b563f9-3117-4f86-83ca-933b0a0b0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972f087b-71ad-462c-997e-4b5bd83b783c",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(combined, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m50\u001b[39m), font, \u001b[38;5;241m0.6\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[0;32m     83\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(combined, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPress \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to quit\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m80\u001b[39m), font, \u001b[38;5;241m0.6\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m---> 85\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassic Methods Visualization\u001b[39m\u001b[38;5;124m'\u001b[39m, combined)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Exit if 'q' is pressed\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Input video file path\n",
    "video_path = r\"D:\\\\Sem-4\\\\Project\\\\Datasets\\\\VISEM-Tracking - Datasets from Paper 2\\\\VISEM_Tracking_Train_v4\\\\Train\\\\11\\\\11.mp4\"\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video.\")\n",
    "    exit()\n",
    "\n",
    "# Background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Kernel for morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Scaling factor to resize the video frames\n",
    "scale_factor = 0.5\n",
    "\n",
    "# Overlay text\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "org_text = (10, 30)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video.\")\n",
    "        break\n",
    "\n",
    "    # Resize the frame\n",
    "    frame_resized = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 1. Grayscale\n",
    "    gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. Background Subtraction\n",
    "    foreground_mask = bg_subtractor.apply(frame_resized)\n",
    "\n",
    "    # 3. Thresholding\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 4. Morphological Operations\n",
    "    opened = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)  # Opening\n",
    "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)  # Closing\n",
    "\n",
    "    # 5. Edge Detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "    # 6. Blurring\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # 7. Contour Detection\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_frame = frame_resized.copy()\n",
    "    cv2.drawContours(contour_frame, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    # Combine all images into a single visualization with labels\n",
    "    top_row = np.hstack([\n",
    "        cv2.putText(frame_resized.copy(), 'Original', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR), 'Grayscale', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(foreground_mask, cv2.COLOR_GRAY2BGR), 'Foreground', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "    middle_row = np.hstack([\n",
    "        cv2.putText(cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR), 'Threshold', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(opened, cv2.COLOR_GRAY2BGR), 'Opened', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(closed, cv2.COLOR_GRAY2BGR), 'Closed', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "    bottom_row = np.hstack([\n",
    "        cv2.putText(cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR), 'Edges', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(blurred, cv2.COLOR_GRAY2BGR), 'Blurred', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(contour_frame, 'Contours', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "\n",
    "    # Stack the combined images\n",
    "    combined = np.vstack([top_row, middle_row, bottom_row])\n",
    "\n",
    "    # Display the combined visualization with file name and quit instructions\n",
    "    file_name = video_path.split(\"\\\\\")[-1]\n",
    "    cv2.putText(combined, f'File: {file_name}', (10, 50), font, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(combined, 'Press \"q\" to quit', (10, 80), font, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Classic Methods Visualization', combined)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Properly release the video and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98f2b9-688d-4994-a3f0-16d301bf126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Input video file path\n",
    "video_path = r\"D:\\Sem-4\\Project\\Papers\\VISEM - Dataset from Paper 1 MP4 format\\1_09.09.02_SSW.mp4\"\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video.\")\n",
    "    exit()\n",
    "\n",
    "# Background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Kernel for morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Tracking parameters\n",
    "tracking_data = {}  # Stores sperm ID and centroid\n",
    "next_sperm_id = 1\n",
    "max_distance = 50  # Maximum distance to consider the same sperm\n",
    "immotile_threshold = 5\n",
    "cluster_threshold = 20\n",
    "history_limit = 10\n",
    "lost_frames_threshold = 10  # Number of frames to keep tracking lost sperms\n",
    "\n",
    "# Helper function to calculate Euclidean distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video.\")\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    fgmask = bg_subtractor.apply(frame)\n",
    "    _, binary = cv2.threshold(fgmask, 200, 255, cv2.THRESH_BINARY)\n",
    "    opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Current frame's detected centroids\n",
    "    current_centroids = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 50:  # Ignore small noise\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cx, cy = int(x + w / 2), int(y + h / 2)  # Centroid of the bounding box\n",
    "        current_centroids.append((cx, cy))\n",
    "\n",
    "    # Update tracking using Hungarian Algorithm\n",
    "    if tracking_data:\n",
    "        existing_ids = list(tracking_data.keys())\n",
    "        existing_centroids = [tracking_data[sperm_id]['centroid'] for sperm_id in existing_ids]\n",
    "\n",
    "        # Compute cost matrix based on Euclidean distance\n",
    "        cost_matrix = np.zeros((len(existing_centroids), len(current_centroids)))\n",
    "        for i, ec in enumerate(existing_centroids):\n",
    "            for j, cc in enumerate(current_centroids):\n",
    "                cost_matrix[i, j] = euclidean_distance(ec, cc)\n",
    "\n",
    "        # Solve the assignment problem\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        # Update tracking data\n",
    "        unmatched_existing = set(range(len(existing_centroids)))\n",
    "        unmatched_current = set(range(len(current_centroids)))\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            if cost_matrix[r, c] < max_distance:\n",
    "                sperm_id = existing_ids[r]\n",
    "                tracking_data[sperm_id]['centroid'] = current_centroids[c]\n",
    "                tracking_data[sperm_id]['lost_frames'] = 0\n",
    "                tracking_data[sperm_id]['history'].append(current_centroids[c])\n",
    "                unmatched_existing.remove(r)\n",
    "                unmatched_current.remove(c)\n",
    "\n",
    "        # Mark unmatched existing IDs as lost\n",
    "        for r in unmatched_existing:\n",
    "            sperm_id = existing_ids[r]\n",
    "            tracking_data[sperm_id]['lost_frames'] += 1\n",
    "\n",
    "        # Add new IDs for unmatched current centroids\n",
    "        for c in unmatched_current:\n",
    "            tracking_data[next_sperm_id] = {\n",
    "                'centroid': current_centroids[c],\n",
    "                'history': [current_centroids[c]],\n",
    "                'immotile': False,\n",
    "                'lost_frames': 0,\n",
    "            }\n",
    "            next_sperm_id += 1\n",
    "    else:\n",
    "        # Initialize tracking with the first frame\n",
    "        for centroid in current_centroids:\n",
    "            tracking_data[next_sperm_id] = {\n",
    "                'centroid': centroid,\n",
    "                'history': [centroid],\n",
    "                'immotile': False,\n",
    "                'lost_frames': 0,\n",
    "            }\n",
    "            next_sperm_id += 1\n",
    "\n",
    "    # Remove lost IDs\n",
    "    tracking_data = {id_: data for id_, data in tracking_data.items() if data['lost_frames'] < lost_frames_threshold}\n",
    "\n",
    "    # Check immotile status\n",
    "    for sperm_id, data in tracking_data.items():\n",
    "        if len(data['history']) >= history_limit:\n",
    "            start_point = data['history'][-history_limit]\n",
    "            end_point = data['centroid']\n",
    "            if euclidean_distance(start_point, end_point) < immotile_threshold:\n",
    "                tracking_data[sperm_id]['immotile'] = True\n",
    "            else:\n",
    "                tracking_data[sperm_id]['immotile'] = False\n",
    "\n",
    "    # Visualization\n",
    "    for sperm_id, data in tracking_data.items():\n",
    "        cx, cy = data['centroid']\n",
    "        x, y, w, h = cv2.boundingRect(np.array([[cx, cy]]))\n",
    "\n",
    "        # Check for clusters\n",
    "        is_cluster = sum(1 for other_id, other_data in tracking_data.items()\n",
    "                         if other_id != sperm_id and euclidean_distance(data['centroid'], other_data['centroid']) < cluster_threshold) > 0\n",
    "\n",
    "        if is_cluster:\n",
    "            color = (0, 0, 255)  # Red for clusters\n",
    "        elif data['immotile']:\n",
    "            color = (255, 0, 0)  # Blue for immotile\n",
    "        else:\n",
    "            color = (0, 255, 0)  # Green for active sperms\n",
    "\n",
    "        # Draw bounding box and label with sperm_tracking_id\n",
    "        cv2.rectangle(frame, (cx - w // 2, cy - h // 2), (cx + w // 2, cy + h // 2), color, 2)\n",
    "        cv2.putText(frame, f\"ID: {sperm_id}\", (cx - 10, cy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Sperm Tracking with Persistent IDs', frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == 27:  # Press 'ESC' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b9745-d6cc-4248-8152-a96d3f537f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import csv\n",
    "\n",
    "# Input video file path\n",
    "video_path = r\"D:\\Sem-4\\Project\\Papers\\VISEM - Dataset from Paper 1 MP4 format\\1_09.09.02_SSW.mp4\"\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video.\")\n",
    "    exit()\n",
    "\n",
    "# Background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Kernel for morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Tracking parameters\n",
    "tracking_data = {}  # Stores sperm ID, centroid, history, etc.\n",
    "next_sperm_id = 1\n",
    "max_distance = 50  # Maximum distance to consider the same sperm\n",
    "immotile_threshold = 5  # Movement threshold for immotile status\n",
    "history_limit = 10\n",
    "lost_frames_threshold = 10  # Number of frames to keep tracking lost sperms\n",
    "frame_rate = 30  # Frame rate of the video (fps)\n",
    "\n",
    "# Output CSV file\n",
    "output_csv = \"sperm_tracking_status.csv\"\n",
    "with open(output_csv, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Frame\", \"Sperm ID\", \"Motility\", \"Velocity (pixels/frame)\", \"Trajectory (x, y)\"])\n",
    "\n",
    "# Helper function to calculate Euclidean distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
    "\n",
    "# Helper function to classify motility\n",
    "def classify_motility(history, velocity):\n",
    "    if velocity < immotile_threshold:\n",
    "        return \"Immotile\"\n",
    "    elif len(history) >= history_limit:\n",
    "        # Check overall directionality (progressive vs. non-progressive)\n",
    "        start_point = history[0]\n",
    "        end_point = history[-1]\n",
    "        net_distance = euclidean_distance(start_point, end_point)\n",
    "        if net_distance > len(history) * immotile_threshold:\n",
    "            return \"Progressive Motile\"\n",
    "        else:\n",
    "            return \"Non-Progressive Motile\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video.\")\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    fgmask = bg_subtractor.apply(frame)\n",
    "    _, binary = cv2.threshold(fgmask, 200, 255, cv2.THRESH_BINARY)\n",
    "    opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Current frame's detected centroids\n",
    "    current_centroids = []\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 50:  # Ignore small noise\n",
    "            continue\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cx, cy = int(x + w / 2), int(y + h / 2)  # Centroid of the bounding box\n",
    "        current_centroids.append((cx, cy))\n",
    "\n",
    "    # Update tracking using Hungarian Algorithm\n",
    "    if tracking_data:\n",
    "        existing_ids = list(tracking_data.keys())\n",
    "        existing_centroids = [tracking_data[sperm_id]['centroid'] for sperm_id in existing_ids]\n",
    "\n",
    "        # Compute cost matrix based on Euclidean distance\n",
    "        cost_matrix = np.zeros((len(existing_centroids), len(current_centroids)))\n",
    "        for i, ec in enumerate(existing_centroids):\n",
    "            for j, cc in enumerate(current_centroids):\n",
    "                cost_matrix[i, j] = euclidean_distance(ec, cc)\n",
    "\n",
    "        # Solve the assignment problem\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        # Update tracking data\n",
    "        unmatched_existing = set(range(len(existing_centroids)))\n",
    "        unmatched_current = set(range(len(current_centroids)))\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            if cost_matrix[r, c] < max_distance:\n",
    "                sperm_id = existing_ids[r]\n",
    "                tracking_data[sperm_id]['centroid'] = current_centroids[c]\n",
    "                tracking_data[sperm_id]['lost_frames'] = 0\n",
    "                tracking_data[sperm_id]['history'].append(current_centroids[c])\n",
    "\n",
    "                # Calculate velocity\n",
    "                prev_position = tracking_data[sperm_id]['history'][-2] if len(tracking_data[sperm_id]['history']) > 1 else current_centroids[c]\n",
    "                tracking_data[sperm_id]['velocity'] = euclidean_distance(prev_position, current_centroids[c]) * frame_rate\n",
    "                unmatched_existing.remove(r)\n",
    "                unmatched_current.remove(c)\n",
    "\n",
    "        # Mark unmatched existing IDs as lost\n",
    "        for r in unmatched_existing:\n",
    "            sperm_id = existing_ids[r]\n",
    "            tracking_data[sperm_id]['lost_frames'] += 1\n",
    "\n",
    "        # Add new IDs for unmatched current centroids\n",
    "        for c in unmatched_current:\n",
    "            tracking_data[next_sperm_id] = {\n",
    "                'centroid': current_centroids[c],\n",
    "                'history': [current_centroids[c]],\n",
    "                'velocity': 0,\n",
    "                'lost_frames': 0,\n",
    "            }\n",
    "            next_sperm_id += 1\n",
    "    else:\n",
    "        # Initialize tracking with the first frame\n",
    "        for centroid in current_centroids:\n",
    "            tracking_data[next_sperm_id] = {\n",
    "                'centroid': centroid,\n",
    "                'history': [centroid],\n",
    "                'velocity': 0,\n",
    "                'lost_frames': 0,\n",
    "            }\n",
    "            next_sperm_id += 1\n",
    "\n",
    "    # Remove lost IDs\n",
    "    tracking_data = {id_: data for id_, data in tracking_data.items() if data['lost_frames'] < lost_frames_threshold}\n",
    "\n",
    "    # Log data to CSV and draw on the frame\n",
    "    with open(output_csv, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for sperm_id, data in tracking_data.items():\n",
    "            motility = classify_motility(data['history'], data['velocity'])\n",
    "            trajectory = data['history']\n",
    "            centroid = data['centroid']\n",
    "            velocity = data['velocity']\n",
    "\n",
    "            # Write data to the CSV file\n",
    "            writer.writerow([cap.get(cv2.CAP_PROP_POS_FRAMES), sperm_id, motility, velocity, trajectory])\n",
    "\n",
    "            # Visualize\n",
    "            color = (0, 255, 0) if motility == \"Progressive Motile\" else (255, 0, 0) if motility == \"Immotile\" else (0, 0, 255)\n",
    "            cv2.circle(frame, centroid, 5, color, -1)\n",
    "            cv2.putText(frame, f\"ID:{sperm_id}\", (centroid[0] + 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Sperm Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == 27:  # Press 'ESC' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d355bae3-7438-4adf-80e7-51c30f342390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\asue\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a3038d-784a-4fe0-9c49-c4776f3d37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\asue\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\asue\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\asue\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dce4a99-6bae-4162-9118-6ea33e9eaee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video.\n",
      "Processing complete. Saved to D:\\\\Sem-4\\\\Project\\\\Processed_Output.avi.\n"
     ]
    }
   ],
   "source": [
    "#To save the processed video\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Input video file path\n",
    "video_path = r\"D:\\\\Sem-4\\\\Project\\\\Datasets\\\\VISEM-Tracking - Datasets from Paper 2\\\\VISEM_Tracking_Train_v4\\\\Train\\\\11\\\\11.mp4\"\n",
    "\n",
    "# Output video file path\n",
    "output_path = r\"D:\\\\Sem-4\\\\Project\\\\Processed_Output.avi\"\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for the output video\n",
    "\n",
    "# Scale down the resolution for processing\n",
    "scale_factor = 0.5\n",
    "output_width = int(frame_width * scale_factor)\n",
    "output_height = int(frame_height * scale_factor)\n",
    "\n",
    "# Initialize VideoWriter for saving the processed video\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (output_width * 3, output_height * 3))  # 3x3 grid\n",
    "\n",
    "# Background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Kernel for morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Overlay text\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video.\")\n",
    "        break\n",
    "\n",
    "    # Resize the frame\n",
    "    frame_resized = cv2.resize(frame, (output_width, output_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 1. Grayscale\n",
    "    gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. Background Subtraction\n",
    "    foreground_mask = bg_subtractor.apply(frame_resized)\n",
    "\n",
    "    # 3. Thresholding\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 4. Morphological Operations\n",
    "    opened = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)  # Opening\n",
    "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)  # Closing\n",
    "\n",
    "    # 5. Edge Detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "    # 6. Blurring\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # 7. Contour Detection\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_frame = frame_resized.copy()\n",
    "    cv2.drawContours(contour_frame, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    # Combine all images into a single visualization\n",
    "    top_row = np.hstack([\n",
    "        cv2.putText(frame_resized.copy(), 'Original', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR), 'Grayscale', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(foreground_mask, cv2.COLOR_GRAY2BGR), 'Foreground', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "    middle_row = np.hstack([\n",
    "        cv2.putText(cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR), 'Threshold', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(opened, cv2.COLOR_GRAY2BGR), 'Opened', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(closed, cv2.COLOR_GRAY2BGR), 'Closed', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "    bottom_row = np.hstack([\n",
    "        cv2.putText(cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR), 'Edges', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(blurred, cv2.COLOR_GRAY2BGR), 'Blurred', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(contour_frame, 'Contours', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "\n",
    "    # Stack the combined images\n",
    "    combined = np.vstack([top_row, middle_row, bottom_row])\n",
    "\n",
    "    # Write to the output video\n",
    "    out.write(combined)\n",
    "\n",
    "# Properly release the video and writer\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Processing complete. Saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d09071-4924-4893-9970-7bead11a35a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([top_row, middle_row, bottom_row])\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Display the combined visualization\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassic Methods Visualization\u001b[39m\u001b[38;5;124m'\u001b[39m, combined)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Exit if 'q' is pressed\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Input video file path\n",
    "video_path = r\"D:\\\\Sem-4\\\\Project\\\\Datasets\\\\VISEM-Tracking - Datasets from Paper 2\\\\VISEM_Tracking_Train_v4\\\\Train\\\\11\\\\11.mp4\"\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video.\")\n",
    "    exit()\n",
    "\n",
    "# Scaling factor to resize the video frames\n",
    "scale_factor = 0.5\n",
    "\n",
    "# Background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Kernel for morphological operations\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "# Overlay text\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video.\")\n",
    "        break\n",
    "\n",
    "    # Resize the frame\n",
    "    frame_resized = cv2.resize(frame, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # 1. Grayscale\n",
    "    gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. Background Subtraction\n",
    "    foreground_mask = bg_subtractor.apply(frame_resized)\n",
    "\n",
    "    # 3. Thresholding\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 4. Morphological Operations\n",
    "    opened = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)  # Opening\n",
    "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)  # Closing\n",
    "\n",
    "    # 5. Edge Detection\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "    # 6. Blurring\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # 7. Contour Detection\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_frame = frame_resized.copy()\n",
    "    cv2.drawContours(contour_frame, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    # Combine all images into a single visualization\n",
    "    top_row = np.hstack([\n",
    "        cv2.putText(frame_resized.copy(), 'Original', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR), 'Grayscale', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(foreground_mask, cv2.COLOR_GRAY2BGR), 'Foreground', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "    middle_row = np.hstack([\n",
    "        cv2.putText(cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR), 'Threshold', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(opened, cv2.COLOR_GRAY2BGR), 'Opened', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(closed, cv2.COLOR_GRAY2BGR), 'Closed', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "    bottom_row = np.hstack([\n",
    "        cv2.putText(cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR), 'Edges', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(cv2.cvtColor(blurred, cv2.COLOR_GRAY2BGR), 'Blurred', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA),\n",
    "        cv2.putText(contour_frame, 'Contours', (10, 30), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    ])\n",
    "\n",
    "    # Stack the combined images\n",
    "    combined = np.vstack([top_row, middle_row, bottom_row])\n",
    "\n",
    "    # Display the combined visualization\n",
    "    cv2.imshow('Classic Methods Visualization', combined)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Properly release the video and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd190f19-087a-40b2-bdb0-fbd1fbe019d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
